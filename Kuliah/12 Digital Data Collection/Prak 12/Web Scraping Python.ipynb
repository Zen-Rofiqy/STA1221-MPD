{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMVtPP4vIPSItKugo6jm47V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"OIHPfHDD97tH","executionInfo":{"status":"ok","timestamp":1668496703183,"user_tz":-420,"elapsed":3447,"user":{"displayName":"Azka Al Azkiya","userId":"02936066482298865106"}}},"outputs":[],"source":["# ORIGINAL CODE\n","import requests\n","import csv\n","from bs4 import BeautifulSoup\n","\n","\n","f = csv.writer(open('z-artist-names.csv', 'w'))\n","f.writerow(['Name', 'Link'])\n","\n","pages = []\n","\n","# Collecting and Parsing a Web Page\n","for i in range(1, 5):\n","    url = 'https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ' + str(i) + '.htm'\n","    pages.append(url)\n","\n","\n","for item in pages:\n","    page = requests.get(item)\n","    soup = BeautifulSoup(page.text, 'html.parser')\n","    \n","    # Remove bottom links\n","    last_links = soup.find(class_='AlphaNav')\n","    last_links.decompose()\n","\n","    artist_name_list = soup.find(class_='BodyText')\n","    artist_name_list_items = artist_name_list.find_all('a')\n","\n","    for artist_name in artist_name_list_items:\n","        names = artist_name.contents[0]\n","        links = 'https://web.archive.org' + artist_name.get('href')\n","\n","        f.writerow([names, links])"]},{"cell_type":"code","source":["# SCRAPING URL DATA\n","import requests\n","import csv\n","from bs4 import BeautifulSoup\n","\n","# Write ke file csv\n","f = csv.writer(open('best-novel.csv', 'w', newline=''))\n","f.writerow(['Title', 'Link'])\n","\n","pages = []\n","\n","# Collecting & parsing konten Web\n","for i in range(1, 5):\n","    url = 'https://www.goodreads.com/list/show/67567.Novel_Indonesia_Terbaik?page=' + str(i)\n","    pages.append(url)\n","\n","for item in pages:\n","    page = requests.get(item)\n","    soup = BeautifulSoup(page.text, 'html.parser')\n","    \n","    # Find elemen class bookTitle di dalam class tableList\n","    novel_title_list = soup.find(class_='tableList')\n","    novel_title_list_items = novel_title_list.find_all(class_='bookTitle')\n","    \n","    # Get masing-masing title dan url dari class tableList\n","    for novel_title in novel_title_list_items:\n","        title = novel_title.find('span',{'itemprop':'name'}).get_text()\n","        link = 'https://www.goodreads.com' + novel_title.get('href')\n","\n","        f.writerow([title, link])"],"metadata":{"id":"EACUHn-y-c5-","executionInfo":{"status":"ok","timestamp":1668496778592,"user_tz":-420,"elapsed":7055,"user":{"displayName":"Azka Al Azkiya","userId":"02936066482298865106"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# SCRAPING REVIEW DATA FROM DIRECT URL\n","import requests\n","import csv\n","from bs4 import BeautifulSoup\n","\n","# Write ke file csv\n","f = csv.writer(open('data-review.csv', 'w', newline=''))\n","f.writerow(['Nama', 'Ulasan'])\n","\n","pages = []\n","\n","url = 'https://www.goodreads.com/book/show/1362193.Laskar_Pelangi' + '?language_code=id'\n","pages.append(url)\n","\n","# Collecting & parsing konten Web\n","for item in pages:\n","    page = requests.get(item)\n","    soup = BeautifulSoup(page.text, 'lxml')\n","    \n","    # Find elemen class review di dalam id bookReviews\n","    novel_review_list = soup.find('div', {'id': 'bookReviews'})\n","    novel_review_list_items = novel_review_list.find_all(class_='review')\n","\n","    # Get masing-masing name dan review dari id bookReviews\n","    for novel_review in novel_review_list_items:\n","        name = novel_review.find(class_='user').get_text()\n","        review = novel_review.find(class_='readable').find('span', recursive=False).get_text()\n","\n","        f.writerow([name, review])"],"metadata":{"id":"9zK7FBlDRuuu","executionInfo":{"status":"ok","timestamp":1668496851179,"user_tz":-420,"elapsed":2248,"user":{"displayName":"Azka Al Azkiya","userId":"02936066482298865106"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# SCRAPING URL ADDRESS\n","import requests\n","import csv\n","from bs4 import BeautifulSoup\n","\n","# Write ke file csv\n","csv_output = csv.writer(open('best-novel-url.csv', 'w', newline=''))\n","\n","pages = []\n","\n","# Collecting & parsing konten Web hlm 1-6\n","for i in range(1, 7):\n","    url = 'https://www.goodreads.com/list/show/67567.Novel_Indonesia_Terbaik?page=' + str(i)\n","    pages.append(url)\n","\n","for item in pages:\n","    page = requests.get(item)\n","    soup = BeautifulSoup(page.text, 'html.parser')\n","    \n","    # Find elemen class bookTitle di dalam class tableList\n","    novel_title_list = soup.find(class_='tableList')\n","    novel_title_list_items = novel_title_list.find_all(class_='bookTitle')\n","    \n","    # Get masing-masing title dan url dari class tableList\n","    for novel_title in novel_title_list_items:\n","        link = 'https://www.goodreads.com' + novel_title.get('href') + '?language_code=id'\n","\n","        csv_output.writerow([link])"],"metadata":{"id":"Byy_1jxvRxBk","executionInfo":{"status":"ok","timestamp":1668496941681,"user_tz":-420,"elapsed":11727,"user":{"displayName":"Azka Al Azkiya","userId":"02936066482298865106"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# SCRAPING REVIEW DATA FROM CSV FILE\n","import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","#Read input url dari file csv & write output review\n","with open('best-novel-url.csv', newline='') as f_urls, open('data-review.csv', 'w', newline='', encoding=\"utf-8\") as f_output:\n","    csv_urls = csv.reader(f_urls)\n","    csv_output = csv.writer(f_output)\n","    csv_output.writerow(['Nama', 'Ulasan', 'Rating'])\n","    \n","    # Collecting & parsing konten web\n","    for line in csv_urls:\n","        r = requests.get(line[0]).text\n","        soup = BeautifulSoup(r, 'lxml')\n","   \n","        # Find elemen class review di dalam id bookReviews\n","        novel_review_list = soup.find('div', {'id': 'bookReviews'})\n","        novel_review_list_items = novel_review_list.find_all(class_='review')\n","        \n","        # Get masing-masing nama & review dari class bookReviews\n","        for novel_review in novel_review_list_items:\n","            name = novel_review.find(class_='user').get_text()\n","            review = novel_review.find(class_='readable').find('span', recursive=False).get_text()\n","            rating_element = novel_review.find('span', {'size': '15x15'})\n","            # Skip item review ketika elemen rating tidak ditemukan\n","            if rating_element == None:                \n","                continue\n","            else :\n","                rating = rating_element.get_text()\n","\n","            csv_output.writerow([name, review, rating])"],"metadata":{"id":"9K6oXi0DR3z3","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1668497008857,"user_tz":-420,"elapsed":6012,"user":{"displayName":"Azka Al Azkiya","userId":"02936066482298865106"}},"outputId":"93ec23a5-e679-4211-87e8-26a69c3d0898"},"execution_count":6,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-a7db29912406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Find elemen class review di dalam id bookReviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnovel_review_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bookReviews'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mnovel_review_list_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnovel_review_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Get masing-masing nama & review dari class bookReviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"]}]}]}